---
lang: en
slug: geospatial-analysis-book
title: The book I start to write about cloud-based geospatial analysis
date:  2017-05-01
tags: book
---
<!-- more -->
![](http://oouh9u8nz.bkt.gdipper.com//geospatial-analysis-book.jpg)

The editors from [packtpub](https://www.packtpub.com/) wrote to me a few weeks ago, asking whether I would like to write a short book about the application of [Google Earth Engine](https://earthengine.google.com/). I am not sure whether I want to do it but I would like to write down some thoughts I have for now.

# Preface
Geospatial analysis is the combination of statistical analysis, computational geometry, and image processing applied to data which is tied to the Earth so far (maybe in the future to other planets). When I was a young student, I started to learn how to do RGB bands combination in GIS software,  like ArcGIS or ERDAS. They are called ‘image processing’ software or ‘spatial modeling’ tools. Because it is exactly what they can do. Right now, I am delightful to see on the homepage of ERDAS; it starts to call itself ‘Geospatial Data Authoring System’ provider. This is a better term, since more and more complex operations and data are included in geospatial analysis. People are trying to process and analyze all kind of the data with spatial information and attributes.

Analyst were depended on expensive commercial applications like ArcGIS, ERDAS or ENVI or packages from open source community (like QGIS, GDAL, etc.), to perform geospatial analysis for more than 20 years. Millions of images have been processed following the same strategy: locate data (through online and physical image archives); acquire data (through internet, FTP, or physical media); process data (usually on a single workstation, only occasionally using parallel programming on high-performance computing (HPC) systems); visualizing and analyzing results (again on a single workstation); and delivering a final product (usually some form of digital map with accompanying metadata and spatial statistics). All these steps required a great deal of time, and it became apparent that often computing power and operational workflows were the limiting factors in large scale geospatial analysis, not datasets or algorithms.
there are still a lot of things to be optimized/visualized beyond processing, like feature exploration, models validation, and tuning. Most of them are still painful tasks not because we don’t have dataset or algorithm, not because we don’t know how to do, but because we don’t know how to make it reproducible, stable and scalable. 

Cloud computing is the buzzword today in the IT world. The most appropriate definition of cloud computing is provided by Borko Furht of Florida Atlantic University, who defines it as "a new style of computing in which dynamically scalable and often virtualized resources are provided as a service over the Internet." Cloud computing is the best way to resolve two obstacles in the geospatial analysis in nowadays: sharing and scaling. 

# Sharing of data and services
The underlying philosophy behind the cloud is the sharing of data and services. There are two approaches to this from users' view: public and private clouds. In a public cloud, the provider has total control of data and services; users can access data and services for realizing their applications. The control on data and services and the right to access are decided by the provider. Public clouds are usually off-premises and run by third parties. Google Maps is perhaps the most visible example of such a public cloud. However, there are many instances where an organization needs to be on the cloud but have security and reliability issues. In such cases, private clouds are the answer. A private cloud is on-premises and run by a wing of the organization. It is accessible to users within that organization. Many people criticize the concept of a private cloud because it provides for none of the benefits of cloud architecture except web access through virtualization. For community clouds, several related organizations can get together and create a cloud infrastructure for their use; a typical system being Google's Apps.gov for US government departments. These are accessed as private, public, hybrid or community cloud services, depending upon the organization’s need for security, collaboration, and ownership.

Google Earth Engine’s public data catalog includes a variety of standard Earth science raster datasets. The most popular ones include Landsat, Sentinel, MODIS,  Climate Dataset and Land Cover Maps. On the cloud, you don’t actually order or download these datasets. Virtually they are available through the online developing platform. You can describe what you plan with these datasets in a short javascript code, manipulate objects on the server by manipulating client-side “proxy” objects. And the real computing will be carried out on the server side. All the traditional functions and library are now available through web-services. And you can also share your codes and output layers to the whole community.

# Scaling 
Scaling, from an IT resource perspective, represents the ability of the IT resource to handle increased or decreased usage demands. People used to build huge computing system to make thing possible. Like [Pleiades Supercomputer](https://www.nas.nasa.gov/hecc/resources/pleiades.html), it represents NASA's state-of-the-art technology for meeting the agency's supercomputing requirements, enabling NASA scientists and engineers to conduct modeling and simulation for NASA missions. If you are an experienced programmer with HPC knowledge, then definitely you can build some strong data pipeline to dig with your massive dataset. However, these computing resources are highly limited to the public because of the learning curve, security requirement and lack of infrastructure. People need a well-defined infrastructure to work with. It should be flexible enough to program to various questions; and also should be well abstracted, so users don’t need to fight with too many details. In the end, scaling is a question highly bound with lower-level implement of the computing system, which means you can’t have an algorithm scalable by simply putting it like that; you have to know how your resources (memory, storage, CPUs) being organized in your system. Which make everything complex. However, a cloud-based geospatial platform means it provides a general solution to these questions so that people can develop on the top of it.

So we can see while traditional GIS is installed on your desktop or server, Cloud GIS makes use of the flexibility of the cloud environment for data capture, visualization, analysis, and sharing. Although GIS has been a late adopter of the cloud technology, the many advantages are compelling organizations to shift their geospatial functions to the cloud. Cloud-based tools are accessed through a web-based geographic information system. There are quite a few of new thing with the cloud-based geospatial analysis. If you are a student or researcher in this field, then undoubtedly you can get some ideas from reading this book, and keep an eye on the changing of this topics. 

I would like to start with a short chapter of overviewing of the general geospatial analysis with different level -- which means, you need to apply different tools for different question level. Most of the time we can find open source tools to our questions. After that we will introduce Google Earth Engine, so far for some specific questions, I only find a solution using it. It is under developing; it is not open source, it looks a huge, ambitious project with a lot of uncertainties -- just like ones people had many times in the history.  But we know, some of them did change the world in the end, in a good way.
